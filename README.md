# Which Questions Should I Ask? Utility Estimation of Questions with LLM-based Simulations
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-green.svg?style=flat-square)](http://makeapullrequest.com)  
[![arXiv](https://img.shields.io/badge/arXiv-2502.17383-b31b1b.svg)](https://arxiv.org/abs/2502.17383)

This repository contains the dataset, models, and evaluation code from our paper:  
**[Which Questions Should I Ask? Utility Estimation of Questions with LLM-based Simulations](https://arxiv.org/abs/2502.17383)**

**Authors**: [Dong-Ho Lee](https://dongholee.com/), [Hyundong Cho](https://justin-cho.com/), [Jonathan May](https://jonmay.github.io/webpage/), [Jay Pujara](https://www.jaypujara.org/)

---

## üìö Overview

Asking effective questions is essential for learning and comprehension. 
However, evaluating and generating useful questions is challenging due to the huge space of possible questions and the lack of direct measures of their impact.

Prior work relies on indirect proxies of question quality, which do not directly assess how much a question helps a learner.  
We introduce **QUEST** (**Q**uestion **U**tility **E**stimation with **S**imulated **T**ests), a simulation framework that quantifies the *utility* of a question, its contribution to learning outcomes, by modeling how it affects a simulated learner's understanding.

**QUEST** identifies high-utility questions and uses them to fine-tune question generation models via rejection sampling.

Across five textbook domains, we find that **QUEST**-trained models produce questions that lead to **20%+ higher exam scores** compared to:
- prompt-based models grounded in instructional design literature, and
- models fine-tuned on indirect quality signals

---

## üìÅ Table of Contents

1. [Setup](#setup)
2. [Data](#data)
3. [Inference](#inference)
4. [Training](#training)

---

## ‚öôÔ∏è Setup

Set your OpenAI API key for inference and training:
```bash
export OPENAI_API_KEY=sk-...
```

---

## üì¶ Data

### `data/data.jsonl`

| Field                  | Description                                             |
|------------------------|---------------------------------------------------------|
| `subject`              | Subject name (e.g., `"chemistry"`)                     |
| `chapter`              | Chapter ID (e.g., `"m50984"`)                           |
| `llm_parsed_results.sections` | Dictionary of section number ‚Üí paragraph content     |
| `llm_parsed_results.questions` | Dict of question info with fields below:            |
| ‚îî‚îÄ `question`          | Question text                                           |
| ‚îî‚îÄ `answer`            | (Optional) Reference answer                             |
| ‚îî‚îÄ `relevant_sections` | List of related section numbers (LLM-inferred)          |

---

### `data/data_post_processed.jsonl`

Generated by:

```bash
python data/postprocess_for_sft_and_few_shot.py
```

| Field     | Description                                                                |
|-----------|----------------------------------------------------------------------------|
| `subject` | Subject name                                                               |
| `section` | Section number the question is based on                                    |
| `question`| Generated question                                                         |
| `answer`  | Generated answer                                                           |
| `context` | Combined content of all previous sections (for few-shot prompting and SFT) |
| `anchor`  | The sentence the student is currently reading (main focus of the question) |

üìå **Usage**: Used for few-shot prompts and SFT (Supervised Fine-Tuning).

## üß™ Inference

To reproduce baseline and model results, run:

```bash
python run_inference.py --subject chemistry physics biology ...
````

You can specify different prompting strategies and models:

| Mode      | Model                 | Description                          |
| --------- | --------------------- | ------------------------------------ |
| `default` | `gpt-4o-mini`         | **Zero-shot** baseline               |
| `cot`     | `gpt-4o-mini`         | **Chain-of-thought** prompting       |
| `fewshot` | `gpt-4o-mini`         | **Few-shot** prompting with examples |
| `default` | `sft-trained-model`   | **SFT** (Supervised Fine-Tuning)     |
| `default` | `quest-trained-model` | **QUEST** (Utility-trained model)    |

Additional options:

* `--num_questions_per_section`: number of questions to generate per section (default: 1)
* `--use_document_for_simulate`: if set, uses full document context during learning simulation

Example usage:

```bash
python run_inference.py \
  --subject chemistry \
  --qg_model_name gpt-4o-mini \
  --evaluate_model_name gpt-4o-mini \
  --mode fewshot
```

Outputs:

* `output/{model_name}/{mode}_performance_results.jsonl`: utility scores for each chapter
* `output/{model_name}/{mode}_qa_pairs.jsonl`: generated question‚Äìanswer pairs
